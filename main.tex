\documentclass[10pt,twocolumn]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{times}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage[hidelinks]{hyperref}
\usepackage{enumitem}
\setlist{noitemsep,leftmargin=*,topsep=2pt}
\sisetup{round-mode=places,round-precision=2}

\title{Challenge 2025 -- ADAMMA:\\
On-Device MET Class Prediction from Smartphone Accelerometers}

\author{Arda Baris Basaran$^{1}$ \\
\small $^{1}$ École Polytechnique Fédérale de Lausanne (EPFL) \\
\small \texttt{ardabarisbasaran@hotmail.com}
}
\date{}

\begin{document}
\maketitle

\begin{abstract}
\noindent
We present a mobile system that continuously estimates a user's Metabolic Equivalent of Task (MET) \emph{class} (Sedentary, Light, Moderate, Vigorous) from smartphone accelerometer data and displays the cumulative time spent in each class throughout the day. The model is trained and evaluated on the WISDM ``Activity Prediction'' dataset, which contains six activities recorded at \SI{20}{Hz} from phones carried in users' front pockets \cite{Kwapisz2010,wisdm_site}. We map activities to MET classes using the Compendium of Physical Activities' conventional intensity thresholds (sedentary 1.0--1.5, light 1.6--2.9, moderate 3.0--5.9, vigorous $\geq$6.0) \cite{Compendium2024,Compendium2011}. Our pipeline uses \textbf{\SI{5}{s}} windows with a \textbf{\SI{1}{s}} stride, a 43-dimensional handcrafted feature set implemented in PyTorch for fast on-device computation, and both classic ML classifiers (logistic regression, kNN, RBF-SVM, random forest, Gaussian NB, and a small MLP) and a compact 1D CNN suitable for on-device deployment. We report standard metrics (accuracy, precision, recall, F1) and per-sample inference latency, and describe an Android foreground-service implementation that maintains near real-time predictions and a clean UI with live cumulative time bars. We release source code (Android app, training code), trained models (including ONNX exports), and scripts to reproduce our results.
\end{abstract}


\section{Introduction}
Daily time spent across intensity classes (sedentary, light, moderate, vigorous) is a strong predictor of cardio-metabolic risk and all-cause mortality. METs (Metabolic Equivalents of Task) provide a simple, widely used scale to relate activity energy cost to resting metabolism. In the Compendium, intensities are typically categorized as sedentary 1.0--1.5 METs, light 1.6--2.9, moderate 3.0--5.9, and vigorous $\geq$6.0 \cite{Compendium2024,Compendium2011}. 

Modern smartphones ship with tri-axial accelerometers and sufficient compute to run lightweight ML models continuously. Our goal is a fully on-device app that (i) infers the current MET \emph{class} in near real time from acceleration alone and (ii) maintains an always-updating tally of cumulative minutes per class for ``today'', with a small, readable UI and low battery overhead.

We build on the WISDM ``Activity Prediction'' dataset \cite{Kwapisz2010,wisdm_site}. While WISDM contains six activity labels---Walking, Jogging, Upstairs, Downstairs, Sitting, Standing---health guidance and the ADAMMA challenge operate in terms of \emph{MET classes}. We therefore adopt a principled mapping from activities to intensity bins, train models to predict the four-class intensity label, and deploy a compact classifier on Android.

\paragraph{Contributions.}
\begin{itemize}
  \item A complete training \& evaluation pipeline for four-class MET intensity recognition using WISDM accelerometer data.
  \item A 43-feature, PyTorch-implemented extractor adapted from the original WISDM feature set \cite{Kwapisz2010}.
  \item Classic ML baselines and a compact 1D CNN, with accuracy/F1 and per-sample latency suitable for on-device use.
  \item An Android architecture (foreground service + live UI) for continuous sensing and cumulative time tracking.
  \item Reproducibility: scripts, configuration, and model artefacts.
\end{itemize}

\section{Methodology}
\subsection{Dataset}
We use the WISDM Activity Prediction dataset \cite{wisdm_site}, collected with Android phones carried in the \emph{front pants pocket} at \SI{20}{Hz} (one sample every \SI{50}{ms}). The canonical WISDM processing derives 43 features on 10\,s windows (\(\approx\)200 samples) \cite{Kwapisz2010}, while our on-device pipeline (below) uses shorter windows for lower latency. The dataset provides six activities: \emph{Walking}, \emph{Jogging}, \emph{Upstairs}, \emph{Downstairs}, \emph{Sitting}, \emph{Standing}. According to the dataset metadata there are 1{,}098{,}207 labeled samples across 36 users (IDs 1\,..\,36) with the following distribution:

\begin{table}[H]
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Activity} & \textbf{Count} & \textbf{Percent} \\
\midrule
Walking   & 424{,}400 & 38.6\% \\
Jogging   & 342{,}177 & 31.2\% \\
Upstairs  & 122{,}869 & 11.2\% \\
Downstairs& 100{,}427 & 9.1\% \\
Sitting   & 59{,}939  & 5.5\% \\
Standing  & 48{,}395  & 4.4\% \\
\bottomrule
\end{tabular}
\end{table}




Each line in the WISDM dataset has the form:

\begin{multline*}
\texttt{[user],[activity],[timestamp],} \\
\texttt{[x-acceleration],[y-accel],[z-accel];}
\end{multline*}

\noindent \textbf{Fields:}
\begin{itemize}[leftmargin=1.1em]
  \item \texttt{user} --- nominal, integers 1\,..\,36.
  \item \texttt{activity} --- nominal in \{\texttt{Walking}, \texttt{Jogging}, \texttt{Sitting}, \texttt{Standing}, \texttt{Upstairs}, \texttt{Downstairs}\}.
  \item \texttt{timestamp} --- numeric; generally phone uptime in nanoseconds (future datasets may use milliseconds since Unix epoch).
  \item \texttt{x-acceleration}, \texttt{y-accel}, \texttt{z-accel} --- floating-point values typically in \([-20,20]\). A value of 10 corresponds to \(1g \approx 9.81\,\mathrm{m/s^2}\); values include gravitational acceleration, so a resting vertical axis registers around \(\pm 10\).
\end{itemize}

\paragraph{Dataset choice.} 
Before committing to WISDM, we considered several other widely used activity recognition datasets. The UCI-HAR dataset \cite{UCIHAR} provides accelerometer and gyroscope signals from a waist-mounted smartphone, pre-segmented into 2.56\,s windows; however, it lacks vigorous-intensity classes (no jogging or running). The MHEALTH dataset \cite{MHEALTH} includes rich multi-sensor streams (ECG, accelerometer, gyroscope, magnetometer) and diverse activities such as cycling and running, but its setup with multiple body-worn devices makes it less representative of a single-phone scenario. PAMAP2 \cite{PAMAP2} similarly offers detailed multi-sensor data and a broad range of activities mapped to METs, yet requires three body-mounted IMUs. More recently, WEEE \cite{WEEE} combines accelerometry with indirect calorimetry for precise energy expenditure, but is not phone-based. Compared to these, WISDM \cite{Kwapisz2010,wisdm_site} is uniquely well aligned with our goals: it uses only the smartphone’s internal accelerometer, sampled at 20\, Hz, in the front pocket carry position. Since our aim is a deployable Android app using just the phone’s own sensors, WISDM provides the most realistic match in terms of modality and placement, while still allowing a principled mapping from activities to MET intensity classes (Table~\ref{tab:met}).

\paragraph{Why classification (not regression).}
We formulate intensity recognition as a \emph{four-class} classification task rather than direct MET regression for several reasons. First, the selected dataset (WISDM) does not provide continuous, ground-truth energy expenditure labels; it contains activity categories only, so any MET value would have to be \emph{imputed} from external tables (e.g., the Compendium) and then treated as truth, introducing label noise and cascading bias \cite{Compendium2024,Compendium2011,wisdm_site}. Second, mapping a regressed MET \emph{value} to intensity requires a second thresholding step (e.g., 1.5/3/6 METs); this two-stage pipeline compounds errors (regression + thresholding), whereas a single classifier can optimize the decision boundaries for the end objective directly. Third, absolute MET varies substantially with person-specific physiology (mass, fitness), biomechanics, and phone placement/orientation—factors that an accelerometer-only model cannot reliably disambiguate without per-user calibration. Treating the problem as classification avoids conveying spurious numerical precision and aligns the model’s outputs with clinical/public-health guidance and our UI.



\paragraph{Our windowing.} For low-latency on-device prediction, we use \textbf{5\,s} windows with a \textbf{1\,s} stride at \SI{20}{Hz} (\(5\times 20=100\) samples per window). This choice, aligned with typical mobile phone sensing setups, balances decision latency with sufficient temporal context to capture periodic structure in gait and stair-climbing, while providing more frequent predictions.


\paragraph{Preprocessing.} We drop malformed or all-zero rows, cast types, and sort by \texttt{user, timestamp}. Windows are generated sequentially per user with a fixed stride (no overlap unless otherwise noted). Each window's label is the \emph{majority} activity over its samples. (Implementation in \texttt{datasets/dataset.py}.)

\subsection{Mapping Activities to MET Classes}
We map WISDM activities to intensity classes using Compendium thresholds \cite{Compendium2024,Compendium2011} and typical MET values:

\begin{table}[H]
\centering
\footnotesize
\begin{tabularx}{\columnwidth}{lX}
\toprule
\textbf{Activity} & \textbf{MET Class (rationale)} \\
\midrule
Sitting, Standing & Sedentary ($<1.5$ MET) \\
Walking (casual) & Light (1.6--2.9 MET) \\
Upstairs, Downstairs & Moderate (3.0--5.9 MET; stair climbing $\sim$4--8) \\
Jogging/Running & Vigorous ($\geq$6.0 MET) \\
\bottomrule
\end{tabularx}
\caption{Activity categories and their MET classes.}
\label{tab:met}
\end{table}

This yields four balanced-by-design intensity labels for the challenge: \emph{Sedentary, Light, Moderate, Vigorous}.

\subsection{Feature Extraction (43 dims)}
We adopt the classic WISDM feature family \cite{Kwapisz2010}, implemented in PyTorch (\texttt{models/model.py}) so the same code can run on CPU/GPU and be ported to mobile if desired. For each window (shape \(T{\times}3\); \(T{=}60\) with our 3\,s windows) and each axis \(x,y,z\):
\begin{itemize}
  \item Mean, standard deviation, and average absolute deviation (3$\times$3 = 9).
  \item Average resultant acceleration \(\mathbb{E}[\sqrt{x^2+y^2+z^2}]\) (1).
  \item Time-between-peaks per axis (3): local-peak detection; average spacing converted to ms via sampling rate.
  \item Binned distributions per axis (10 bins $\times$ 3 = 30).
\end{itemize}
Total \(9+1+3+30=43\) features per window. The extractor is window-length agnostic.

\subsection{Models}
We evaluate classic scikit-learn classifiers and a compact neural network suitable for mobile deployment:
\begin{itemize}
  \item \textbf{Logistic Regression} (max\_iter=1000).
  \item \textbf{kNN} ($k=5$).
  \item \textbf{RBF-SVM}.
  \item \textbf{Random Forest} (100 trees).
  \item \textbf{Gaussian Naive Bayes}.
  \item \textbf{MLP} (hidden layers 128$\rightarrow$64, ReLU, Adam, max\_iter=200).
  \item \textbf{SimpleCNN1D} (ours): three depthwise-separable 1D convolution blocks with dilations 1/2/4 and ReLU6, global average pooling, and a small dense head (64$\rightarrow$32$\rightarrow$4). Trained with cross-entropy using Adam (lr $10^{-3}$), dropout 0.1, for 10 epochs.
\end{itemize}

\paragraph{Training protocol.} By default we use stratified 5-fold cross-validation \textit{over windows} (Scikit's \texttt{StratifiedKFold}, random\_state=42). For fold \(k\), we train on 4 folds and test on the held-out fold. The same protocol is applied to both ML models and the CNN. \emph{Note:} this is not subject-exclusive; we discuss the implications in Section~\ref{sec:discussion}. 

\paragraph{Metrics and latency.} We report accuracy, weighted precision/recall/F1, and \emph{per-sample} inference time (mean over the test set: batch predict \(\Rightarrow\) wall time / \#samples, expressed in ms/sample). Code in \texttt{main.py}. For the CNN, we measure forward-pass wall time in evaluation mode.

\subsection{Device Specifications}
\label{sec:device-specs}
\paragraph{Build environment.}
The Android app is developed in Android Studio Narwhal 3 Feature Drop 2025.1.3, Android Gradle Plugin (AGP) 8.5.2, Gradle 8.7, with \texttt{compileSdk=34}, \texttt{targetSdk=34}, and \texttt{minSdk=24}.

\paragraph{Test device.}
We tested end-to-end sensing and the UI on a Samsung Galaxy A15. The app requests a \SI{20}{Hz} accelerometer sampling rate; on this hardware, the sensor reports the closest supported rate, approximately \SI{25}{Hz}. The window size and stride are set to \textbf{5\,s} and \textbf{1\,s}, respectively, matching the training setup.


\section{App Design}
\label{sec:app-design}
The Android app follows a simple, readable flow:
\begin{itemize}
  \item \textbf{Splash Screen:} Lightweight splash while the app initializes services and loads model assets.
  \item \textbf{Starter Screen:} Shows the app logo and a prominent \emph{Start} button to begin sensing.
  \item \textbf{Activity Monitor:} The main screen presents the \emph{current state} (Sedentary/Light/Moderate/Vigorous) with an icon and label. A compact \emph{Debug Information} panel displays live accelerometer values (x, y, z), the measured sampling rate in Hz, the model's output probability distribution over the four classes, and the window/stride configuration in seconds. These values update in real time as new sensor data arrives.
  \item \textbf{History Screen:} A four-tab view (\emph{Day/Week/Month/Year}) summarizing time spent per intensity. The Day tab shows, for each class, a 24-segment bar (one per hour) and a \emph{pie chart} of today's distribution, plus a \emph{calories today (net)} estimate computed from per-second labels and user profile (age, weight, height, sex). Week/Month/Year aggregate into 7/30/12 segments respectively. A \emph{Reset Data} action in Settings clears all history.
\end{itemize}

\subsection{Calorie Estimation}\label{sec:calorie}

\paragraph{What a MET means.}
A Metabolic Equivalent of Task (MET) is a unit that scales energy cost relative to quiet rest.
By convention,
\[
1~\mathrm{MET} \;\triangleq\; 3.5~\mathrm{ml~O_2~kg^{-1}~min^{-1}}
\]
and gross caloric expenditure (including resting) can be estimated from METs and body mass.\footnote{The factor $3.5$~ml/kg/min and the caloric equivalent $1~\mathrm{L~O_2}\!\approx\!5~\mathrm{kcal}$ lead to the $200$ constant below.}
Importantly, MET$\to$kcal depends primarily on \emph{body mass}; height/age/sex mainly matter through basal metabolic rate (BMR), which is separate from activity energy.

\paragraph{From class posteriors to a per-second MET.}
Our on-device model outputs, each second $t$, a posterior over four MET classes
\[
\mathbf{p}_t \;=\; \big[p_t^{(\text{sed})},~p_t^{(\text{light})},~p_t^{(\text{mod})},~p_t^{(\text{vig})}\big],\quad \sum_c p_t^{(c)}=1.
\]
We assign each class a representative MET intensity $m_c$ based on the Compendium ranges and our activity mapping (\emph{sedentary:} sitting/standing; \emph{light:} walking; \emph{moderate:} stairs; \emph{vigorous:} jogging). We use mid-range values for stability:
\[
m_{\text{sed}}=1.2,\quad
m_{\text{light}}=2.5,\quad
m_{\text{mod}}=4.5,\quad
m_{\text{vig}}=8.0.
\]
The per-second \emph{expected} MET is the posterior mean:
\[
\widehat{M}_t \;=\; \sum_{c\in\{\text{sed,light,mod,vig}\}} p_t^{(c)}\, m_c.
\]
To reduce jitter while remaining responsive, we apply an exponential moving average (EMA) with smoothing parameter $\alpha\in(0,1]$:
\[
\widetilde{M}_t \;=\; \alpha\,\widehat{M}_t + (1-\alpha)\,\widetilde{M}_{t-1},\qquad \widetilde{M}_0=\widehat{M}_0.
\]

\paragraph{Gross vs.\ net calories (personalized).}
Let $W$ be body mass (kg), $H$ be height (cm), $A$ age (years), and $S$ a sex indicator ($+5$ for male, $-161$ for female).
We first compute resting metabolic rate (RMR) using the Mifflin–St Jeor equation:
\[
\mathrm{RMR} = 10W + 6.25H - 5A + S \quad [\mathrm{kcal/day}].
\]
Converting to seconds:
\[
\dot{E}_{\mathrm{rest}} = \frac{\mathrm{RMR}}{24 \cdot 3600} \quad [\mathrm{kcal/s}].
\]
This defines a personalized baseline: $1\,\text{MET} = \dot{E}_{\mathrm{rest}}$.

Thus, the instantaneous gross and net caloric rates are
\[
\dot{E}^{\text{gross}}_t = \widetilde{M}_t \cdot \dot{E}_{\mathrm{rest}},
\qquad
\dot{E}^{\text{net}}_t = \max(\widetilde{M}_t-1,\,0) \cdot \dot{E}_{\mathrm{rest}}.
\]
Daily totals are obtained by summation:
\[
E^{(\cdot)}_{\text{day}} = \sum_{t\in \text{day}} \dot{E}^{(\cdot)}_t.
\]

\paragraph{Cumulative time in classes.}
Although we estimate a continuous MET each second, the leaderboard metric requires time per class.
We accumulate \emph{expected} time in each class by summing posteriors:
\[
T_c = \sum_{t\in \text{day}} p_t^{(c)} \cdot 1~\mathrm{s},
\]
so that $\sum_c T_c$ equals the elapsed day time even when the model is uncertain. For history views where a hard label is desired, we can use $\arg\max_c p_t^{(c)}$; expected time is more statistically efficient.

\paragraph{Units sanity check (worked example).}
Example: $W=70$~kg, $H=175$~cm, $A=25$~years, male ($S=+5$).  
Then
\[
\mathrm{RMR} = 10\cdot70 + 6.25\cdot175 - 5\cdot25 + 5 = 1673.75~\mathrm{kcal/day},
\]
\[
\dot{E}_{\mathrm{rest}} = \frac{1673.75}{86400} \approx 0.0194~\mathrm{kcal/s}.
\]
For $\widetilde{M}_t=8$ (vigorous jog):
\[
\dot{E}^{\text{gross}}_t = 8 \cdot 0.0194 \approx 0.155~\mathrm{kcal/s} \;\;(\approx 9.3~\mathrm{kcal/min}),
\]
which closely matches standard tables for jogging intensity.


\section{Results}
\label{sec:results}
We evaluate the six classic classifiers \emph{and} the compact 1D CNN under identical 5-fold CV. Unless noted, handcrafted features (43-D) with window length \SI{5}{s} at \SI{20}{Hz} are used. We report \emph{mean} over folds.

To enable a fair comparison across modeling approaches, we report performance using a consistent set of evaluation metrics. Each table entry is formatted as \((\text{Acc}, \text{Prec}, \text{Rec}, \text{F1}, \text{Inf. time})\).

\begin{table}[H]
\centering
\scriptsize
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Acc (\%)} & \textbf{Prec (\%)} & \textbf{Rec (\%)} & \textbf{F1 (\%)} & \textbf{Inf. time (ms)} \\
\midrule
Logistic Regression & 85.4 $\pm$ 0.2 & 84.9 $\pm$ 0.2 & 85.4 $\pm$ 0.2 & 84.9 $\pm$ 0.2 & 0.0003 \\
kNN (k=5)           & 76.2 $\pm$ 0.4 & 75.8 $\pm$ 0.3 & 76.2 $\pm$ 0.4 & 75.4 $\pm$ 0.4 & 0.0119 \\
RBF-SVM             & 75.1 $\pm$ 0.2 & 75.4 $\pm$ 0.4 & 75.1 $\pm$ 0.2 & 69.2 $\pm$ 0.3 & 1.8195 \\
Random Forest       & 98.0 $\pm$ 0.1 & 98.0 $\pm$ 0.1 & 98.0 $\pm$ 0.1 & 98.0 $\pm$ 0.1 & 0.0096 \\
Gaussian NB         & 84.0 $\pm$ 0.3 & 84.0 $\pm$ 0.3 & 84.0 $\pm$ 0.3 & 83.9 $\pm$ 0.3 & 0.0006 \\
MLP (128,64)        & 96.8 $\pm$ 0.5 & 96.9 $\pm$ 0.5 & 96.8 $\pm$ 0.5 & 96.8 $\pm$ 0.5 & 0.0015 \\
SimpleCNN1D         & 90.6 $\pm$ 1.2 & 91.5 $\pm$ 1.0 & 90.6 $\pm$ 1.2 & 90.8 $\pm$ 1.1 & 1.8067 \\
\bottomrule
\end{tabular}%
}
\caption{5-fold CV performance using handcrafted 43-D features (5\,s windows, 1\,s stride). Metrics are mean $\pm$ std across folds (in \%), inference time reported as mean $\pm$ std (ms/sample).}
\label{tab:handcrafted}
\end{table}


\begin{table}[H]
\centering
\scriptsize
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Acc (\%)} & \textbf{Prec (\%)} & \textbf{Rec (\%)} & \textbf{F1 (\%)} & \textbf{Inf. time (ms)} \\
\midrule
Logistic Regression & 56.7 $\pm$ 0.1 & 60.2 $\pm$ 0.5 & 56.7 $\pm$ 0.1 & 53.5 $\pm$ 0.2 & 0.001 \\
kNN (k=5)           & 72.2 $\pm$ 0.3 & 81.0 $\pm$ 0.3 & 72.2 $\pm$ 0.3 & 72.3 $\pm$ 0.3 & 0.057 \\
RBF-SVM             & 93.5 $\pm$ 0.4 & 93.5 $\pm$ 0.4 & 93.5 $\pm$ 0.4 & 93.4 $\pm$ 0.4 & 5.223 \\
Random Forest       & 88.5 $\pm$ 0.2 & 89.6 $\pm$ 0.2 & 88.5 $\pm$ 0.2 & 87.3 $\pm$ 0.3 & 0.015 \\
Gaussian NB         & 62.3 $\pm$ 0.3 & 68.2 $\pm$ 0.3 & 62.3 $\pm$ 0.3 & 63.0 $\pm$ 0.2 & 0.003 \\
MLP (128,64)        & 94.7 $\pm$ 0.3 & 94.7 $\pm$ 0.3 & 94.7 $\pm$ 0.3 & 94.7 $\pm$ 0.3 & 0.002 \\
SimpleCNN1D         & 99.0 $\pm$ 0.2 & 99.0 $\pm$ 0.2 & 99.0 $\pm$ 0.2 & 99.0 $\pm$ 0.2 & 0.1702 \\
\bottomrule
\end{tabular}%
}
\caption{5-fold CV performance using raw features (5\,s windows, 1\,s stride). Metrics are mean $\pm$ std across folds (in \%), inference time reported as mean $\pm$ std (ms/sample).}
\label{tab:raw}
\end{table}

\paragraph{Latency.} For each model, we measure test-time mean inference latency (ms/sample) using batch prediction, then divide wall time by the number of test windows (see \texttt{main.py}). For the CNN, latency is measured with the network in evaluation mode (BatchNorm/Dropout frozen). Classic scikit-learn classifiers are timed on CPU, whereas the CNN is timed on GPU; these are offline desktop measurements and \emph{not} on-device estimates.

\section{Future Work}
\label{sec:future}
\paragraph{Pre-filtering (orientation + gravity robustness).}
Maintain an IIR low-pass estimate of gravity per axis (cutoff $\approx$ 0.25\, Hz), then compute linear acceleration: $\mathbf{g}_t = \alpha\,\mathbf{g}_{t-1} + (1-\alpha)\,\mathbf{a}_t$, $\mathbf{a}_{\text{lin}} = \mathbf{a}_t - \mathbf{g}_t$, with $\alpha = \exp(-2\pi f_c / f_s)$. Also compute the magnitude $m=\lVert\mathbf{a}_{\text{lin}}\rVert=\sqrt{x^2+y^2+z^2}$ for orientation-invariant cues.

\paragraph{Feature set extensions (per window, from $\mathbf{a}_{\text{lin}}$).}
Time-domain, fast, orientation-robust features that can complement or replace raw inputs:
\begin{itemize}
  \item Per axis (x,y,z) and magnitude $m$: mean, std, MAD $\Rightarrow$ 4 signals $\times$ 3 stats $=16$.
  \item Interquartile range (IQR, $p_{75}-p_{25}$) for x,y,z,$m$ $\Rightarrow$ 4.
  \item Zero-crossing rate (after mean removal) for x,y,z,$m$ $\Rightarrow$ 4.
  \item Signal Magnitude Area (SMA) $= \tfrac{1}{N}\sum_{t}(|x|+|y|+|z|)$ $\Rightarrow$ 1.
  \item Energy $= \text{mean}(x^2),\,\text{mean}(y^2),\,\text{mean}(z^2),\,\text{mean}(m^2)$ $\Rightarrow$ 4.
  \item Jerk stats on first difference $\Delta a$: std for x,y,z,$m$ $\Rightarrow$ 4.
\end{itemize}

\paragraph{Temporal smoothing $\Rightarrow$ stable MET classes.}
Per-window predictions may wiggle. Apply lightweight smoothing directly to the class probabilities with an exponential moving average (EMA): $\hat{\mathbf{p}}_t = \alpha\, \mathbf{p}_t + (1-\alpha)\, \hat{\mathbf{p}}_{t-1}$, with $\alpha \approx 0.7$. Combine with simple hysteresis on class switches: require $k$ consecutive frames (e.g., $k=2$--$3$) before committing a new class to the UI/history. This yields steadier labels without masking offline errors.


\paragraph{Cross-dataset evaluation.}
To assess generalization beyond WISDM and reduce dataset-specific bias, we can train/test on additional public benchmarks: \emph{UCI-HAR}, \emph{MHEALTH}, \emph{PAMAP2}, and \emph{WEEE}. We can harmonize sampling rates and windowing, map activities to MET classes, and report subject-exclusive results where possible. Models (43-D features and raw 1D CNN) can be re-tuned per dataset and evaluated for cross-dataset transfer.


\paragraph{Model architectures beyond a simple 1D CNN.}
Due to the limited time available in this challenge, we restricted ourselves to basic classifiers and a simple 1D CNN. With more time, we would conduct a focused literature review on human activity recognition and efficient on-device time-series modeling, and evaluate additional architectures that better capture long-range temporal structure and multi-scale patterns while remaining lightweight:
\begin{itemize}
  \item \textbf{Recurrent models:} LSTM/GRU with sequence-to-label pooling or temporal attention.
  \item \textbf{Temporal Convolutional Networks (TCN):} Dilated causal convolutions (WaveNet-style) for long receptive fields.
  \item \textbf{CNN--RNN hybrids:} DeepConvLSTM and variants that combine convolutional feature extractors with recurrent back-ends.
  \item \textbf{Multi-scale CNNs:} InceptionTime/ResNet1D/FCN-style architectures with parallel kernels and residual connections.
  \item \textbf{Transformer-based encoders:} Lightweight or linear-attention transformers (e.g., Performer/Informer) and other efficient time-series transformers with positional encodings suitable for on-device inference.
  \item \textbf{Self-supervised pretraining:} Contrastive style objectives (e.g., SimCLR variants) on unlabeled motion data followed by a small classifier head.
\end{itemize}
We would also explore model compression for deployment (post-training quantization, pruning, etc) and architecture scaling under strict latency/energy budgets on typical smartphones.




\section{Discussion}
\label{sec:discussion}

\paragraph{Handcrafted vs. raw windows.}
Tables~\ref{tab:handcrafted} and~\ref{tab:raw} show a consistent pattern: apart from RBF--SVM and the 1D CNN, most classic models perform better with the \emph{handcrafted} 43-D features than with \emph{raw} windows. This is expected: engineered statistics reduce dimensionality, normalize scale, and summarize periodic structure, which benefits linear models (LR), tree ensembles (RF), Naive Bayes, and small MLPs. In contrast, training these models directly on raw, high-dimensional sequences suffers from the curse of dimensionality and a lack of built-in temporal invariances.

\paragraph{Why SVM and CNN behave differently.}
The RBF--SVM improves on raw windows, likely because the kernel can capture complex, nonlinear similarities in the original sequence space without relying on human-designed summaries. The 1D CNN performs \emph{best} on raw inputs: convolution and pooling layers learn local, shift-tolerant patterns (e.g., gait cycles) and aggregate them over time, which aligns naturally with accelerometer time series. When forced onto handcrafted features, CNNs lose their main inductive bias and bring less value.

\paragraph{On-device choice: compact 1D CNN on raw inputs.}
We deploy an ONNX-exported 1D CNN trained on raw windows in the Android app because it offers the best trade-off for our goals:
\begin{itemize}
  \item \textbf{Accuracy:} Highest cross-validated accuracy/F1 among evaluated models on raw data.
  \item \textbf{Latency/size:} Small parameter count, fast forward pass; packaged once as a single \texttt{.onnx} file.
  \item \textbf{Simplicity:} Avoids replicating an exact, numerically identical handcrafted feature pipeline on-device; fewer moving parts and less drift risk.
  \item \textbf{Robustness:} Convolutions provide translation tolerance in time; the model generalizes across minor sampling-rate variation (we feed [1, $N\!\times\!3$] with measured $N$).
\end{itemize}
For flexibility, our app can also accept a 43-D feature model (detected via the input shape of the ONNX model) if future experiments favor that path.



\section{Software \& Reproducibility}
\paragraph{Repository layout.}
\begin{itemize}
  \item \texttt{android-app/}: Kotlin app with foreground service, SensorManager, feature computation, and UI.
  \item \texttt{training/}: your \texttt{main.py}, \texttt{datasets/}, \texttt{models/}.
  \item \texttt{onnx\_models/}: exported models for deployment (scikit-learn via skl2onnx, CNN via PyTorch ONNX export).
  \item \texttt{report/}: this paper.
\end{itemize}

\paragraph{Running training.}
Example (handcrafted features, classic ML):
\begin{verbatim}
python main.py \
  --dataset WISDM \
  --dataset_path /path/to/WISDM/ \
  --feature_extraction handcrafted \
  --model ml \
  --n_splits 5 \
\end{verbatim}
Run the compact CNN by switching \texttt{--model cnn}; use either handcrafted features or raw windows:
\begin{verbatim}
python main.py \
  --dataset WISDM \
  --dataset_path /path/to/WISDM/ \
  --feature_extraction raw \
  --model cnn \
  --n_splits 5 \
  --batch_size 32
\end{verbatim}


\section{Conclusion}
We implement a compact, on-device MET-class recognizer trained on WISDM accelerometer data, using 43 handcrafted features with light ML models \emph{and} a compact 1D CNN to balance accuracy, latency, and battery. A foreground service drives continuous sensing and a clear UI for cumulative daily minutes by intensity. Future work includes subject-exclusive evaluation, stronger temporal models (e.g., temporal CNNs/TCN, lightweight transformers), and robustness to diverse phone placements.


\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}
